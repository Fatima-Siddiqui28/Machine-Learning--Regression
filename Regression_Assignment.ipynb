{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vtbtz9sTHy2S"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q.1  What is Simple Linear Regression ?\n",
        "Ans. Simple Linear Regression is a supervised learning technique in statistics and machine learning that models the relationship between two continuous variables:\n",
        "\n",
        "Independent variable (X): The input or cause.\n",
        "\n",
        "Dependent variable (Y): The output or effect you're trying to predict.\n",
        "\n",
        "It tries to fit a straight line through the data points that best describes how Y changes with x.\n",
        "\n",
        "\n",
        "Q.2 What are the key assumptions of Simple Linear Regression ?\n",
        "Ans. Here are the 5 main assumptions:\n",
        "\n",
        "1.  Linearity- The relationship between the independent variable x\n",
        "X and the dependent variable Y is linear.\n",
        "\n",
        "This means the expected change in Y is proportional to the change in x.\n",
        "\n",
        "Check with: scatterplot, residual plot (residuals should be randomly scattered).\n",
        "\n",
        "2. Independence of Errors (Residuals)-The errors (differences between observed and predicted values) are independent of each other.\n",
        "\n",
        "Especially important for time series data‚Äîerrors shouldn't follow a pattern.\n",
        "\n",
        "Check with: Durbin-Watson test (for autocorrelation in residuals).\n",
        "\n",
        "\n",
        "3. Homoscedasticity (Constant Variance of Errors)- The variance of the errors is the same across all levels of the independent variable.\n",
        "\n",
        "Violations (heteroscedasticity) can affect the accuracy of confidence intervals and p-values.\n",
        "\n",
        "Check with: residuals vs. fitted values plot‚Äîresiduals should have constant spread.\n",
        "\n",
        "\n",
        "4. Normality of Errors- The errors (residuals) should be normally distributed, especially for small sample sizes.\n",
        "\n",
        "Important for hypothesis testing and constructing confidence intervals.\n",
        "\n",
        "Check with: histogram or Q-Q plot of residuals.\n",
        "\n",
        "\n",
        "5. No Perfect Multicollinearity (only applies in multiple regression)\n",
        "For simple linear regression (with one independent variable), this isn't relevant.\n",
        "\n",
        "But if you expand to multiple linear regression, ensure predictors aren‚Äôt perfectly correlated.\n",
        "\n",
        "\n",
        "\n",
        "Q.3 What does the coefficient m represent in the equation Y=mX+c ?\n",
        "Ans. In the simple linear regression equation:\n",
        "               Y=mX+c\n",
        "    the coefficient ùëö represents the slope of the regression line. It tells you:\n",
        "              m is the rate of change in Y for every one-unit increase in X.\n",
        "\n",
        "              It shows the strength and direction of the relationship between(independent variable) and Y (dependent variable).\n",
        "\n",
        "\n",
        "\n",
        "Q.4  What does the intercept c represent in the equation Y=mX+c ?\n",
        "Ans. In the equation of a straight line:\n",
        "               Y = mX+c\n",
        "the intercept c represents the value of Y when the independent variable X=0.\n",
        "\n",
        "\n",
        "\n",
        "Q.5  What is the purpose of the least squares method in Simple Linear Regression ?\n",
        "Ans. The least squares method is used in simple linear regression to-\n",
        "     Find the straight line that best fits your data. It does this by\n",
        "     Making the total prediction error as small as possible\n",
        "(that is, the difference between the actual values and what the line predicts).\n",
        "\n",
        "In Simple Terms:\n",
        "It's like trying to balance the line so it's not too high or too low for the points.\n",
        "\n",
        "The line it gives you is the one that makes the overall error the smallest.\n",
        "\n",
        "\n",
        "Q.6 What is Multiple Linear Regression ?\n",
        "Ans. Multiple Linear Regression (MLR) is a statistical method used to model the relationship between one dependent variable (Y) and two or more independent variables (X‚ÇÅ, X‚ÇÇ, X‚ÇÉ, ...).\n",
        "\n",
        "While simple linear regression uses one input to predict an outcome, multiple linear regression uses several inputs to make a more accurate prediction.\n",
        "\n",
        "\n",
        "\n",
        "Q.7 What is the main difference between Simple and Multiple Linear Regression ?\n",
        "Ans. The key difference lies in the number of independent variables used to predict the dependent variable.\n",
        "- Simple Linear Regression involves just one independent variable. It models the relationship between a single predictor and the outcome using a straight line.\n",
        "- Multiple Linear Regression extends this concept by incorporating two or more independent variables to improve prediction accuracy. It considers multiple factors that might influence the dependent variable, allowing for a more complex and realistic representation.\n",
        "\n",
        "Think of it like predicting someone's salary: using simple linear regression, you might only consider their years of experience. But with multiple linear regression, you'd also factor in their education, location, and industry to refine the prediction.\n",
        "\n",
        "\n",
        "\n",
        "Q.8 What are the key assumptions of Multiple Linear Regression ?\n",
        "Ans. Multiple Linear Regression (MLR) relies on several key assumptions to ensure accurate predictions and meaningful interpretations. Here's what needs to hold true:\n",
        "- Linearity- The relationship between independent variables and the dependent variable should be linear.\n",
        "- Independence- Observations should be independent of each other.\n",
        "- Homoscedasticity-The variance of errors should remain constant across all levels of independent variables.\n",
        "- No Multicollinearity-Independent variables shouldn't be too highly correlated with each other.\n",
        "- Normality of Residuals-The errors (residuals) should be approximately normally distributed.\n",
        "- No Autocorrelation- Residuals should not be correlated with each other (especially important in time-series data).\n",
        "\n",
        "\n",
        "Q.9 What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model ?\n",
        "Ans. Heteroscedasticity occurs when the variance of residuals (errors) is not constant across all levels of the independent variables in a Multiple Linear Regression model. Essentially, it means that the spread of errors changes as values of predictors change‚Äîcreating inconsistent variability.\n",
        "\n",
        "Effects on the Model :\n",
        "\n",
        "1. Unreliable Predictions-Since the errors fluctuate unpredictably,   estimates of the dependent variable may not be as precise.\n",
        "\n",
        "2. Inefficient Estimates-Ordinary Least Squares (OLS) regression assumes homoscedasticity, so heteroscedasticity can make coefficient estimates inefficient.\n",
        "\n",
        "3. Biased Standard Errors- This can distort hypothesis testing (like t-tests and F-tests), leading to misleading conclusions.\n",
        "\n",
        "4. Incorrect Confidence Intervals-Due to distorted standard errors, confidence intervals around estimates may be too narrow or too wide.\n",
        "\n",
        "5. Impact on Model Fit-If heteroscedasticity is severe, it may indicate a need for transformation or alternative regression techniques (e.g., Weighted Least Squares or Generalized Least Squares).\n",
        "\n",
        "\n",
        "\n",
        "Q.10 How can you improve a Multiple Linear Regression model with high multicollinearity ?\n",
        "Ans. If your Multiple Linear Regression model is suffering from high multicollinearity, meaning independent variables are too strongly correlated with each other, it can distort coefficient estimates and make interpretation difficult. Here's how you can improve the model:\n",
        "\n",
        "- Remove Highly Correlated Predictors- Identify variables with strong correlation (using a correlation matrix or Variance Inflation Factor (VIF)) and eliminate redundant ones.\n",
        "\n",
        "- Use Principal Component Analysis (PCA)- PCA transforms correlated variables into a smaller set of uncorrelated components, preserving the overall information.\n",
        "\n",
        "- Apply Ridge Regression (L2 Regularization)- This penalizes large coefficients, reducing the impact of multicollinearity while maintaining all predictors.\n",
        "\n",
        "- Use Lasso Regression (L1 Regularization)-Lasso can shrink some coefficients to zero, effectively selecting only the most relevant predictors.\n",
        "\n",
        "- Increase Sample Size- A larger dataset can help stabilize estimates, making multicollinearity less problematic.\n",
        "\n",
        "- Combine or Transform Variables- Instead of keeping two highly correlated predictors separate, you can create an interaction term or transform them into a single feature.\n",
        "\n",
        "\n",
        "Q.11 What are some common techniques for transforming categorical variables for use in regression models ?\n",
        "Ans. Transforming categorical variables for regression models is crucial because regression techniques typically require numerical inputs. Here are some common methods:\n",
        "- One-Hot Encoding- Converts each category into a separate binary (0/1) column. Ideal for nominal variables without order (e.g., color: red, blue, green).\n",
        "\n",
        "- Label Encoding- Assigns numeric labels to categories (e.g., \"Small\" = 1, \"Medium\" = 2, \"Large\" = 3). Works well for ordinal variables with an inherent order.\n",
        "\n",
        "- Dummy Variables-Similar to one-hot encoding but removes one category to prevent dummy variable trap (perfect collinearity in regression).\n",
        "\n",
        "- Target Encoding- Replaces categories with the mean of the dependent variable for each category. Risky due to potential data leakage.\n",
        "\n",
        "- Frequency Encoding- Assigns values based on how often a category appears in the dataset. Works best for categorical variables with many levels.\n",
        "\n",
        "- Binary Encoding- Converts categories into binary format and assigns them numeric values, reducing dimensionality compared to one-hot encoding.\n",
        "\n",
        "- Mean Encoding- Uses the mean of the target variable within each category as the new value. Can be powerful but may introduce bias.\n",
        "\n",
        "- Ordinal Encoding-If the categories have a clear ranking (e.g., \"Low,\" \"Medium,\" \"High\"), assign numeric values based on hierarchy.\n",
        "\n",
        "\n",
        "\n",
        "Q.12 What is the role of interaction terms in Multiple Linear Regression ?\n",
        "Ans. Interaction terms in Multiple Linear Regression help capture the effect of two or more independent variables interacting with each other, rather than affecting the dependent variable separately. They allow the model to account for situations where the relationship between one predictor and the outcome depends on the value of another predictor.\n",
        "\n",
        "Why Use Interaction Terms?\n",
        "- Capturing Complex Relationships- Some variables don't just have separate effects; they modify each other's impact.\n",
        "\n",
        "- Improving Model Accuracy- Ignoring interactions can lead to oversimplified conclusions.\n",
        "\n",
        "- Avoiding Misinterpretation-Without interaction terms, one variable's effect might seem constant, even when it changes based on another variable.\n",
        "\n",
        "\n",
        "Q.13  How can the interpretation of intercept differ between Simple and Multiple Linear Regression ?\n",
        "Ans. The interpretation of the intercept differs between Simple Linear Regression and Multiple Linear Regression because of the way the model accounts for independent variables\n",
        "1. Simple Linear Regression (SLR)\n",
        "- The intercept represents the predicted value of the dependent variable when the independent variable equals zero.\n",
        "- Example: In a model predicting salary based on years of experience, the intercept would be the estimated salary for someone with zero years of experience.\n",
        "2. Multiple Linear Regression (MLR)\n",
        "- The intercept here is more complex because it represents the predicted value when all independent variables are set to zero.\n",
        "- However, in practice, this interpretation can be meaningless or unrealistic‚Äîespecially if zero values for all predictors don‚Äôt make sense (e.g., zero education, zero work experience).\n",
        "- The intercept is often not the focus of interpretation, and predictions are typically made based on actual variable values rather than relying on the intercept.\n",
        "\n",
        "\n",
        "\n",
        "Q.14 What is the significance of the slope in regression analysis, and how does it affect predictions?\n",
        "Ans.The slope in regression analysis plays a critical role in understanding how changes in the independent variable(s) influence the dependent variable.\n",
        "Significance of the Slope\n",
        "- Rate of Change- The slope quantifies how much the dependent variable changes for each unit increase in an independent variable.\n",
        "- Direction of Relationship- A positive slope means that as the independent variable increases, the dependent variable also increases.\n",
        "- A negative slope indicates that as the independent variable increases, the dependent variable decreases.\n",
        "- Strength of Effect- The magnitude of the slope determines how strong the relationship is. A larger absolute value suggests a stronger impact.\n",
        "\n",
        "\n",
        "Q.15  How does the intercept in a regression model provide context for the relationship between variables ?\n",
        "Ans.The intercept in a regression model serves as a baseline reference point, providing context for how the independent variables relate to the dependent variable.\n",
        "Role of the Intercept:\n",
        "- Baseline Value- It represents the predicted value of the dependent variable when all independent variables are equal to zero.\n",
        "- Starting Point for Predictions- Every regression equation begins with the intercept, which acts as a foundation before adjusting for the effects of predictors.\n",
        "- Contextual Meaning- In some models, the intercept has practical significance (e.g., initial cost before adding extra features). In other cases, it might be a theoretical value that lacks real-world relevance.\n",
        "\n",
        "  Simple vs. Multiple Regression Context:\n",
        "- In Simple Linear Regression, the intercept is straightforward‚Äîit shows the expected outcome when the predictor is zero.\n",
        "- In Multiple Linear Regression, it reflects the baseline value when all independent variables are held at zero, which may not always be realistic.\n",
        "\n",
        "\n",
        "\n",
        "Q.16 What are the limitations of using R¬≤ as a sole measure of model performance?\n",
        "\n",
        "Ans. While R¬≤ (coefficient of determination) is a useful metric for assessing how well a regression model explains the variability in the dependent variable, relying on it alone can be misleading. Here's why:\n",
        "\n",
        "Limitations of R¬≤:\n",
        "- Does Not Indicate Causality ‚Äì A high R¬≤ doesn‚Äôt prove that independent variables directly cause changes in the dependent variable.\n",
        "- Sensitive to Model Complexity ‚Äì Adding more predictors always increases R¬≤, even if the new variables don‚Äôt actually improve the model.\n",
        "- Ignores Overfitting ‚Äì A very high R¬≤ might suggest the model is too complex and overfitting the training data, making it unreliable for new data.\n",
        "- Cannot Evaluate Model Accuracy ‚Äì It shows how well the model fits the data but doesn‚Äôt tell how precise the predictions are.\n",
        "- Not Suitable for Non-Linear Relationships ‚Äì If the true relationship isn‚Äôt linear, R¬≤ might not capture the pattern correctly.\n",
        "- Affected by Outliers ‚Äì Extreme values can distort R¬≤, making the model appear better or worse than it actually is.\n",
        "- Fails to Assess Individual Predictor Impact ‚Äì A high R¬≤ doesn‚Äôt mean all independent variables are important; some may be insignificant\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Q.17 How would you interpret a large standard error for a regression coefficient ?\n",
        "Ans. A large standard error for a regression coefficient suggests greater uncertainty in the estimated value of that coefficient. Here‚Äôs what it means:\n",
        "Key Interpretations:\n",
        "- Less Confidence in the Estimate ‚Äì A large standard error indicates that the coefficient might vary significantly across different samples, making it less reliable.\n",
        "- Weak Predictor Influence ‚Äì It often suggests the independent variable is not strongly affecting the dependent variable.\n",
        "- High Variability in Data ‚Äì If there‚Äôs a lot of scatter or inconsistency in the relationship between the predictor and outcome, the standard error increases.\n",
        "- Possible Multicollinearity ‚Äì In Multiple Linear Regression, multicollinearity (high correlation among independent variables) can inflate standard errors, affecting interpretability.\n",
        "- Impact on Hypothesis Testing ‚Äì A large standard error may lead to wide confidence intervals and a higher p-value, making it difficult to conclude whether the coefficient is statistically significant.\n",
        "\n",
        "\n",
        "\n",
        "Q.18 How can heteroscedasticity be identified in residual plots, and why is it important to address it ?\n",
        "Ans. Identifying Heteroscedasticity in Residual Plots\n",
        "Heteroscedasticity appears when residuals do not have a constant variance across the range of predicted values. Here's how to detect it using residual plots:\n",
        "\n",
        "- Residual vs. Fitted Values Plot ‚Äì Look for a fan-shaped pattern where residuals spread out as fitted values increase.\n",
        "- Scatterplot of Residuals ‚Äì If residuals form a cone or show systematic variation, it‚Äôs a sign of heteroscedasticity.\n",
        "- Breusch-Pagan Test ‚Äì A formal statistical test to check for changing error variance.\n",
        "- White Test ‚Äì Another test that detects non-constant variance in residuals.\n",
        "- Histogram of Residuals ‚Äì If the spread of residuals changes across different sections of the histogram, heteroscedasticity may be present.\n",
        "\n",
        "Why Is It Important to Address?\n",
        "\n",
        "- Biased Standard Errors ‚Äì Affects confidence intervals and hypothesis tests.\n",
        "- Inefficient Estimates ‚Äì Makes regression coefficients unreliable.\n",
        "- Poor Model Predictions ‚Äì The model may fail to generalize well.\n",
        "- Misleading Significance Tests ‚Äì Can lead to incorrect conclusions about predictor importance.\n",
        "\n",
        "\n",
        "\n",
        "Q.19  Why is it important to scale variables in Multiple Linear Regression?\n",
        "\n",
        "Ans. Scaling variables in Multiple Linear Regression is crucial, especially when your predictors have vastly different ranges or units. Here‚Äôs why it matters:\n",
        "Importance of Scaling:\n",
        "- Avoids Numerical Instability ‚Äì Large differences in variable magnitudes can lead to unstable coefficient estimates.\n",
        "- Improves Interpretability ‚Äì Standardized coefficients help compare the relative importance of predictors.\n",
        "- Enhances Model Convergence ‚Äì Gradient-based optimization methods (especially in complex models) perform better with scaled data.\n",
        "- Reduces Bias in Regularization Techniques ‚Äì Methods like Ridge and Lasso Regression work best when variables are scaled.\n",
        "- Prevents Domination by Large-Scale Variables ‚Äì If predictors have vastly different ranges (e.g., age vs. income), unscaled models might overemphasize the one with larger values.\n",
        "\n",
        "\n",
        "\n",
        "Q.20 What is polynomial regression?\n",
        "\n",
        "Ans. Polynomial Regression is an extension of linear regression that models the relationship between the independent and dependent variables using a polynomial equation rather than a simple straight line.\n",
        "\n",
        "Why Use Polynomial Regression?\n",
        "- Captures Non-Linear Relationships ‚Äì When data shows a curved trend, polynomial regression fits better than a simple linear model.\n",
        "- Improves Prediction Accuracy ‚Äì Helps in scenarios where a straight-line assumption leads to poor results.\n",
        "- Useful in Various Fields ‚Äì Found in physics, economics, finance, and machine learning applications.\n",
        "\n",
        "\n",
        "\n",
        "Q.21 How does polynomial regression differ from linear regression ?\n",
        "\n",
        "Ans. Polynomial Regression extends linear regression by adding polynomial terms to capture non-linear relationships. Instead of fitting a straight line (Y = Œ≤‚ÇÄ + Œ≤‚ÇÅX), it models curves using higher-degree terms (Y = Œ≤‚ÇÄ + Œ≤‚ÇÅX + Œ≤‚ÇÇX¬≤ + ‚Ä¶ + Œ≤‚ÇôX‚Åø).\n",
        "Key Differences from Linear Regression:\n",
        "- Flexibility: Polynomial regression fits curves, while linear regression assumes a straight-line relationship.\n",
        "- Complexity: Higher-degree polynomials allow for more intricate patterns but risk overfitting.\n",
        "- Interpretation: Effects vary across the range of X, making interpretation trickier than simple linear regression.\n",
        "\n",
        "\n",
        "\n",
        "Q.22 When is polynomial regression used ?\n",
        "\n",
        "Ans. Polynomial regression is used when a linear model doesn‚Äôt adequately capture the relationship between variables. It helps model non-linear patterns while still using regression techniques.\n",
        "Common Applications:\n",
        "- Growth and Decay Modeling ‚Äì Tracking population growth, radioactive decay, or financial trends.\n",
        "- Physics and Engineering ‚Äì Describing curved motion, material stress analysis, or wave propagation.\n",
        "- Economics and Finance ‚Äì Predicting fluctuating market trends or demand curves.\n",
        "- Machine Learning ‚Äì Enhancing predictive models when relationships aren't strictly linear.\n",
        "\n",
        "\n",
        "\n",
        "Q.23 - What is the general equation for polynomial regression ?\n",
        "\n",
        "Ans. The general equation for Polynomial Regression extends a simple linear model by including higher-degree terms of the independent variable:\n",
        "[ Y = \\beta_0 + \\beta_1 X + \\beta_2 X3 + ... + \\beta_n X^n + \\varepsilon ]\n",
        "Where:\n",
        "- (Y) is the dependent variable.\n",
        "- (X) is the independent variable.\n",
        "- (\\beta_0) is the intercept.\n",
        "- (\\beta_1, \\beta_2, \\beta_3, ..., \\beta_n) are the coefficients for each polynomial term.\n",
        "- (\\varepsilon) represents the error term.\n",
        "The degree ((n)) determines the complexity‚Äîhigher degrees allow for more flexibility but can lead to overfitting\n",
        "\n",
        "\n",
        "\n",
        "Q.24 Can polynomial regression be applied to multiple variables ?\n",
        "\n",
        "Ans. Yes! Polynomial regression can be applied to multiple variables, extending the model beyond a single predictor. This is known as multivariate polynomial regression and allows for capturing non-linear relationships between multiple independent variables and the dependent variable.\n",
        "General Equation for Multivariate Polynomial Regression:\n",
        "[ Y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\beta_3X_12 + \\beta_5X_1X_2 + ... + \\varepsilon ] Where:\n",
        "- ( X_1, X_2, ... ) are independent variables.\n",
        "- Terms like ( X_12 ) introduce quadratic effects.\n",
        "- ( X_1X_2 ) represents an interaction term, capturing how the variables jointly affect ( Y ).\n",
        "\n",
        "Why Use Multivariate Polynomial Regression?\n",
        "- Captures non-linearity between multiple features.\n",
        "- Improves prediction accuracy when relationships are complex.\n",
        "- Useful for real-world applications like economics, engineering, and machine learning.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Q.25 What are the limitations of polynomial regression?\n",
        "\n",
        "Ans. While Polynomial Regression is great for modeling non-linear relationships, it does come with some limitations:\n",
        "- Overfitting Risk ‚Äì Higher-degree polynomials can fit noise in the data rather than actual trends, reducing generalizability.\n",
        "- Interpretability Issues ‚Äì Complex polynomial models make it harder to understand the impact of individual predictors.\n",
        "- Extrapolation Problems ‚Äì Predictions outside the observed range can be highly unreliable.\n",
        "- Increased Computational Complexity ‚Äì Higher-degree polynomials require more computations and may slow down processing.\n",
        "- Collinearity Between Terms ‚Äì Squared and higher-order terms often correlate with their linear counterparts, affecting stability.\n",
        "- Sensitive to Outliers ‚Äì Polynomial models tend to amplify the effect of extreme values.\n",
        "\n",
        "\n",
        "\n",
        "Q.26 What methods can be used to evaluate model fit when selecting the degree of a polynomial ?\n",
        "\n",
        "ANs. When choosing the degree of a polynomial in regression, it's important to ensure the model balances accuracy and complexity. Here are key methods to evaluate model fit:\n",
        "- Adjusted R¬≤ ‚Äì Unlike regular R¬≤, it accounts for the number of predictors, helping prevent overfitting.\n",
        "- Cross-Validation ‚Äì Splitting data into training and test sets ensures the model generalizes well.\n",
        "- Mean Squared Error (MSE) / Root Mean Squared Error (RMSE) ‚Äì Measures overall prediction error; lower values indicate better fit.\n",
        "- Akaike Information Criterion (AIC) / Bayesian Information Criterion (BIC) ‚Äì Penalizes overly complex models to prevent overfitting.\n",
        "- Residual Analysis ‚Äì Checking plots of residuals can reveal patterns suggesting if a higher degree is needed.\n",
        "- Polynomial Degree vs. Error Trade-off ‚Äì Plotting test error across different degrees helps identify the optimal complexity.\n",
        "\n",
        "\n",
        "\n",
        "Q.27  Why is visualization important in polynomial regression ?\n",
        "\n",
        "Ans. Visualization is crucial in Polynomial Regression because it helps in understanding the complex relationships between variables and evaluating model performance. Here‚Äôs why it matters:\n",
        "- Detects Non-Linearity ‚Äì Scatter plots reveal whether a polynomial curve fits the data better than a straight line.\n",
        "- Identifies Overfitting ‚Äì High-degree polynomials can create overly complex curves that match training data but fail to generalize; visualizing test performance helps spot this.\n",
        "- Residual Analysis ‚Äì Plotting residuals helps check whether errors are randomly distributed or show patterns that indicate a poor fit.\n",
        "- Choosing Polynomial Degree ‚Äì Visualizing different polynomial curves allows for a better balance between underfitting and overfitting.\n",
        "- Interpreting Model Behavior ‚Äì Graphs help communicate how changes in independent variables affect predictions.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JoNGBr-rH2Tw"
      }
    }
  ]
}